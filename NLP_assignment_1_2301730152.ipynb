{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Take a custom paragraph, perform the entire pipeline and Print results at each step.\n",
        "\n",
        "# a)Tokenization → b)Stopword Removal → c)Stemming → d)Lemmatization."
      ],
      "metadata": {
        "id": "i9dATinzdwTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "bmtYYPDnAEbB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ImWZF16AL3B",
        "outputId": "dc8f9e15-2213-4c45-b6c1-b564a639a48f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom paragraph\n",
        "custom_para = \"\"\"The universe is incredibly vast and mysterious; astronomers are constantly observing distant galaxies, searching for exoplanets, and developing powerful new telescopes. Analyzing all this incoming space data requires sophisticated techniques, like efficient tokenization and careful lemmatization, to truly unlock the cosmos's secrets.\"\"\"\n",
        "\n",
        "print(\"Original Paragraph:\")\n",
        "print(custom_para)\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sVBs2XuA8hJ",
        "outputId": "bd534420-55ed-4aa1-d525-b03c34d1a56f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "The universe is incredibly vast and mysterious; astronomers are constantly observing distant galaxies, searching for exoplanets, and developing powerful new telescopes. Analyzing all this incoming space data requires sophisticated techniques, like efficient tokenization and careful lemmatization, to truly unlock the cosmos's secrets.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a)Tokenization"
      ],
      "metadata": {
        "id": "yRkT0hoGd5TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Tokenization\n",
        "tokens = word_tokenize(custom_para)\n",
        "print(\"a) Tokenization:\")\n",
        "print(tokens)\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsZuzXyUA8q5",
        "outputId": "027f9845-e56a-4e6b-df25-93c749c1cda6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) Tokenization:\n",
            "['The', 'universe', 'is', 'incredibly', 'vast', 'and', 'mysterious', ';', 'astronomers', 'are', 'constantly', 'observing', 'distant', 'galaxies', ',', 'searching', 'for', 'exoplanets', ',', 'and', 'developing', 'powerful', 'new', 'telescopes', '.', 'Analyzing', 'all', 'this', 'incoming', 'space', 'data', 'requires', 'sophisticated', 'techniques', ',', 'like', 'efficient', 'tokenization', 'and', 'careful', 'lemmatization', ',', 'to', 'truly', 'unlock', 'the', 'cosmos', \"'s\", 'secrets', '.']\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b)Stopword Removal"
      ],
      "metadata": {
        "id": "Xbw7wX_8eEHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "print(\"b) Stopword Removal:\")\n",
        "print(filtered_tokens)\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21ztkLJA8wE",
        "outputId": "f289161d-9f9e-4299-f59d-3e79cacac6b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b) Stopword Removal:\n",
            "['universe', 'incredibly', 'vast', 'mysterious', 'astronomers', 'constantly', 'observing', 'distant', 'galaxies', 'searching', 'exoplanets', 'developing', 'powerful', 'new', 'telescopes', 'Analyzing', 'incoming', 'space', 'data', 'requires', 'sophisticated', 'techniques', 'like', 'efficient', 'tokenization', 'careful', 'lemmatization', 'truly', 'unlock', 'cosmos', 'secrets']\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c)Stemming"
      ],
      "metadata": {
        "id": "u-VBFajoeMT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"c) Stemming:\")\n",
        "print(stemmed_tokens)\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhrjOVluA8yc",
        "outputId": "649592a1-127d-43ef-cf71-4e4fb889199b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c) Stemming:\n",
            "['univers', 'incred', 'vast', 'mysteri', 'astronom', 'constantli', 'observ', 'distant', 'galaxi', 'search', 'exoplanet', 'develop', 'power', 'new', 'telescop', 'analyz', 'incom', 'space', 'data', 'requir', 'sophist', 'techniqu', 'like', 'effici', 'token', 'care', 'lemmat', 'truli', 'unlock', 'cosmo', 'secret']\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d)Lemmatization"
      ],
      "metadata": {
        "id": "iR1-9yjQeROl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"d) Lemmatization:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWEC4rMPA819",
        "outputId": "2232a9b3-c3e7-4ddd-9c86-4bfc08bf8552"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d) Lemmatization:\n",
            "['universe', 'incredibly', 'vast', 'mysterious', 'astronomer', 'constantly', 'observing', 'distant', 'galaxy', 'searching', 'exoplanets', 'developing', 'powerful', 'new', 'telescope', 'Analyzing', 'incoming', 'space', 'data', 'requires', 'sophisticated', 'technique', 'like', 'efficient', 'tokenization', 'careful', 'lemmatization', 'truly', 'unlock', 'cosmos', 'secret']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Define NLP and its real time application in a specific domain base.\n",
        "\n",
        "-->Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) that gives computers the ability to read, understand, interpret, and generate human language (both spoken and written).\n",
        "\n",
        "The core goal of NLP is to bridge the gap between human communication—which is highly unstructured, ambiguous, and contextual—and machine understanding, which requires structured, logical data. NLP combines concepts from computer science, AI, and linguistics to achieve this, allowing machines to derive meaning, determine sentiment, and even generate natural, human-like responses.\n",
        "\n",
        "Real-Time NLP in the Financial Domain: Market Sentiment Analysis\n",
        "\n",
        "A prominent application of real-time NLP is in the Financial Services domain, specifically for Market Sentiment Analysis.\n",
        "\n",
        "Specific Domain: Finance\n",
        "\n",
        "In finance, quick access to information is critical. Every day, trillions of dollars' worth of trades are influenced by news, regulatory filings, social media, and earnings call transcripts. Analyzing this vast, unstructured text data manually is impossible.\n",
        "\n",
        "Real-Time Application: Market Sentiment Analysis\n",
        "\n",
        "Market Sentiment Analysis uses NLP to automatically determine the tone or emotion expressed toward a specific company, stock, or market sector in real-time.\n",
        "\n",
        "1. How it Works (Real-Time):\n",
        "\n",
        "    Data Ingestion: The NLP system continuously streams text data from multiple sources:\n",
        "\n",
        "        Financial news wires (e.g., Bloomberg, Reuters).\n",
        "\n",
        "        Social media (e.g., relevant financial subreddits, X/Twitter feeds).\n",
        "\n",
        "        Regulatory filings (e.g., SEC 8-K reports for immediate company news).\n",
        "\n",
        "        Live transcripts of CEO speeches or earnings calls.\n",
        "\n",
        "    Preprocessing & Analysis: As the text streams in, the following NLP tasks are performed in milliseconds:\n",
        "\n",
        "        Tokenization & Entity Recognition: Identifying key tokens and tagging financial entities (e.g., \"TSLA\" as a company, \"$250M\" as a currency amount, \"acquisition\" as a corporate action).\n",
        "\n",
        "        Sentiment Classification: An NLP model (often a deep learning model like a Transformer) classifies the text as Positive, Negative, or Neutral. The system is trained on financial-specific lexicon (e.g., in finance, words like \"volatile\" or \"risk\" are often highly negative, while \"growth\" and \"exceeds\" are positive).\n",
        "\n",
        "        Aggregation: The individual sentiment scores are aggregated to provide a real-time sentiment score for a particular asset.\n",
        "\n",
        "2. Real-Time Impact:\n",
        "\n",
        "    Algorithmic Trading (HFT): High-Frequency Trading (HFT) firms use these real-time sentiment signals to make automated buy or sell decisions. For example, if a sentiment score for a company suddenly plunges due to a breaking news story, an algorithm can automatically sell the stock within milliseconds, before human traders can even read the headline.\n",
        "\n",
        "    Risk Management: Compliance teams can monitor internal and external communications in real-time to detect potentially risky or non-compliant language, flagging possible insider trading or unauthorized activity.\n",
        "\n",
        "    Investment Decision Support: Portfolio managers receive real-time alerts that summarize the public mood towards their holdings, allowing them to adjust strategies immediately based on breaking information.\n",
        "\n",
        "\n",
        "# 3) What is NLU and NLG?\n",
        "\n",
        "-->**Natural Language Understanding (NLU)** : It is the specialized component within Natural Language Processing (NLP) that handles the challenge of interpreting human input. Its primary goal is to extract the meaning, intent, and context from user text or speech, even when the language is complex, ambiguous, or contains errors. NLU achieves this by performing crucial tasks like identifying the user's ultimate goal (intent recognition) and pulling out critical information (entity recognition), effectively serving as the machine's \"reading comprehension\" system.\n",
        "\n",
        "**Natural Language Generation (NLG)** : It is the equally vital counterpart that focuses on creating human-like language output. Its function is to take the structured, machine-ready data—the answer or action determined by the system—and translate it back into a coherent, grammatically correct, and contextually appropriate response that a human can easily understand. NLG focuses on the mechanics of articulation, including sentence structuring and choosing the right words, thereby handling the machine's outbound communication (formulating and speaking)."
      ],
      "metadata": {
        "id": "0Q5qGVQieYId"
      }
    }
  ]
}